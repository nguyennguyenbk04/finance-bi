{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a35920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:33.563432Z",
     "iopub.status.busy": "2025-10-10T04:44:33.563260Z",
     "iopub.status.idle": "2025-10-10T04:44:34.197487Z",
     "shell.execute_reply": "2025-10-10T04:44:34.196943Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610df923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:34.199579Z",
     "iopub.status.busy": "2025-10-10T04:44:34.199262Z",
     "iopub.status.idle": "2025-10-10T04:44:34.202594Z",
     "shell.execute_reply": "2025-10-10T04:44:34.202106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "MINIO_ENDPOINT = 'http://minio:9000'\n",
    "MINIO_ACCESS_KEY = 'minioadmin'\n",
    "MINIO_SECRET_KEY = 'minioadmin123'\n",
    "MYSQL_HOST = 'mysql'\n",
    "MYSQL_PORT = '3306'\n",
    "MYSQL_DATABASE = 'finance_dw'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASSWORD = 'root123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211b202d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:34.204412Z",
     "iopub.status.busy": "2025-10-10T04:44:34.204214Z",
     "iopub.status.idle": "2025-10-10T04:44:43.703550Z",
     "shell.execute_reply": "2025-10-10T04:44:43.702803Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "com.mysql#mysql-connector-j added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4c4862d0-ef05-4d8f-b911-94cccd28efcf;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.4.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.6 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      ":: resolution report :: resolve 325ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.6 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4c4862d0-ef05-4d8f-b911-94cccd28efcf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/9ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 04:44:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"DataWarehouse-ETL\")\n",
    "    # Memory configurations for ETL processing\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    # Jars for Delta Lake, S3, and MySQL\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262,\" \\\n",
    "    \"io.delta:delta-spark_2.13:4.0.0,\" \\\n",
    "    \"com.mysql:mysql-connector-j:8.0.33\")\n",
    "    # Delta Lake\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # MinIO (S3A) - Source\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    # S3A performance configs\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"3\")\n",
    "    .config(\"spark.hadoop.fs.s3a.retry.limit\", \"3\")\n",
    ")\n",
    "\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5bfaa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:43.706098Z",
     "iopub.status.busy": "2025-10-10T04:44:43.705738Z",
     "iopub.status.idle": "2025-10-10T04:44:43.709732Z",
     "shell.execute_reply": "2025-10-10T04:44:43.709235Z"
    }
   },
   "outputs": [],
   "source": [
    "dw_mysql_url = f\"jdbc:mysql://{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}\"\n",
    "dw_mysql_properties = {\n",
    "    \"user\": MYSQL_USER,\n",
    "    \"password\": MYSQL_PASSWORD,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# tables_config = {\n",
    "#         'users': {\n",
    "#             'path': 's3a://warehouse/dim_user/',\n",
    "#             'partitions': None\n",
    "#         },\n",
    "#         'mcc_codes': {\n",
    "#             'path': 's3a://warehouse/dim_mcc/',\n",
    "#             'partitions': None\n",
    "#         },\n",
    "#         'cards': {\n",
    "#             'path': 's3a://warehouse/dim_card/',\n",
    "#             'partitions': ['card_brand']  # Partition by brand for better queries\n",
    "#         },\n",
    "#         'transactions': {\n",
    "#             'path': 's3a://warehouse/fact_transactions/',\n",
    "#             'partitions': ['year', 'month']  # Partition by date for performance\n",
    "#         },\n",
    "#         'fraud_labels': {\n",
    "#             'path': 's3a://warehouse/fraud_labels/',\n",
    "#             'partitions': None\n",
    "#         }\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ff79c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:43.711349Z",
     "iopub.status.busy": "2025-10-10T04:44:43.711190Z",
     "iopub.status.idle": "2025-10-10T04:44:55.442108Z",
     "shell.execute_reply": "2025-10-10T04:44:55.441356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 04:44:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 04:44:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                        (0 + 16) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:=================>                                      (16 + 16) / 50]\r",
      "\r",
      "[Stage 2:===================================>                    (32 + 16) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: 2000 rows loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcc_codes: 109 rows loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cards: 6146 rows loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions: 13305915 rows loaded\n"
     ]
    }
   ],
   "source": [
    "users_df = spark.read.format(\"delta\").load(\"s3a://rootdb/users/\")\n",
    "mcc_codes_df = spark.read.format(\"delta\").load(\"s3a://rootdb/mcc_codes/\")\n",
    "cards_df = spark.read.format(\"delta\").load(\"s3a://rootdb/cards/\")\n",
    "transactions_df = spark.read.format(\"delta\").load(\"s3a://rootdb/transactions/\")\n",
    "\n",
    "print(f\"users: {users_df.count()} rows loaded\")\n",
    "print(f\"mcc_codes: {mcc_codes_df.count()} rows loaded\")\n",
    "print(f\"cards: {cards_df.count()} rows loaded\")\n",
    "print(f\"transactions: {transactions_df.count()} rows loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9e291b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:55.444419Z",
     "iopub.status.busy": "2025-10-10T04:44:55.444204Z",
     "iopub.status.idle": "2025-10-10T04:44:58.440403Z",
     "shell.execute_reply": "2025-10-10T04:44:58.439661Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 04:44:57 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to dim_mcc/part-00000-4c259f72-10f6-4e50-8363-0ee986986c45-c000.snappy.parquet. This is Unsupported\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# POPULATE dim_mcc\n",
    "dim_mcc_df = mcc_codes_df.select(\n",
    "    col(\"mcc\"),\n",
    "    col(\"merchant_type\")\n",
    ")\n",
    "\n",
    "# dim_mcc_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_mcc\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_mcc_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://lakehouse/dim_mcc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac5e8a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:44:58.443096Z",
     "iopub.status.busy": "2025-10-10T04:44:58.442852Z",
     "iopub.status.idle": "2025-10-10T04:45:02.290434Z",
     "shell.execute_reply": "2025-10-10T04:45:02.289636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:>                                                       (0 + 16) / 16]\r",
      "\r",
      "[Stage 39:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:=====================>                                  (6 + 10) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 44:===>                                                    (1 + 15) / 16]\r",
      "\r",
      "[Stage 44:================================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# POPULATE dim_date\n",
    "\n",
    "min_date = transactions_df.select(min(\"trans_date\")).collect()[0][0]\n",
    "max_date = transactions_df.select(max(\"trans_date\")).collect()[0][0]\n",
    "\n",
    "date_range_df = spark.sql(f\"\"\"\n",
    "    SELECT sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day) as date_array\n",
    "\"\"\").select(explode(col(\"date_array\")).alias(\"full_date\"))\n",
    "\n",
    "dim_date_df = date_range_df.select(\n",
    "    # Create surrogate key as YYYYMMDD integer\n",
    "    date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_sk\"),\n",
    "    col(\"full_date\"),\n",
    "    dayofmonth(col(\"full_date\")).alias(\"day\"),\n",
    "    weekofyear(col(\"full_date\")).alias(\"week\"),\n",
    "    month(col(\"full_date\")).alias(\"month\"),\n",
    "    quarter(col(\"full_date\")).alias(\"quarter\"),\n",
    "    year(col(\"full_date\")).alias(\"year\"),\n",
    "    dayofweek(col(\"full_date\")).alias(\"day_of_week\"),\n",
    "    when(dayofweek(col(\"full_date\")).isin([1, 7]), 1).otherwise(0).alias(\"is_weekend\")\n",
    ")\n",
    "\n",
    "# dim_date_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_date\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_date_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://lakehouse/dim_date/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9719ae3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:45:02.293019Z",
     "iopub.status.busy": "2025-10-10T04:45:02.292681Z",
     "iopub.status.idle": "2025-10-10T04:45:03.516832Z",
     "shell.execute_reply": "2025-10-10T04:45:03.516191Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. POPULATE dim_users\n",
    "\n",
    "# Create customer dimension with surrogate keys\n",
    "dim_user_df = users_df.select(\n",
    "    col(\"client_id\"),\n",
    "    col(\"client_id\").alias(\"user_sk\"),  # Using natural key as surrogate for now\n",
    "    col(\"birth_year\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"yearly_income\"),\n",
    "    col(\"total_debt\"),\n",
    "    col(\"credit_score\"),\n",
    "    col(\"num_credit_cards\"),\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\"),\n",
    "    # Create user segment based on credit score\n",
    "    when(col(\"credit_score\") >= 750, \"Premium\")\n",
    "    .when(col(\"credit_score\") >= 650, \"Standard\")\n",
    "    .when(col(\"credit_score\") >= 550, \"Subprime\")\n",
    "    .otherwise(\"High Risk\").alias(\"user_segment\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "# dim_user_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_user\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_user_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://lakehouse/dim_user/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f20657b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:45:03.519244Z",
     "iopub.status.busy": "2025-10-10T04:45:03.518976Z",
     "iopub.status.idle": "2025-10-10T04:45:24.973317Z",
     "shell.execute_reply": "2025-10-10T04:45:24.972562Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate merchant_ids in transactions: 53611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:=======>                                                (2 + 14) / 16]\r",
      "\r",
      "[Stage 66:==============>                                         (4 + 12) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:========================>                                (7 + 9) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique merchants found: 211495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 76:>               (0 + 16) / 16][Stage 77:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 76:=>              (1 + 15) / 16][Stage 77:>                 (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 76:=======>                                                (2 + 14) / 16]\r",
      "\r",
      "[Stage 76:=====================>                                  (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 76:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 79:====================================================>   (16 + 1) / 17]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates after join: 16182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:>               (0 + 16) / 16][Stage 92:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:=>              (1 + 15) / 16][Stage 92:>                 (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:===>                                                    (1 + 15) / 16]\r",
      "\r",
      "[Stage 91:=======>                                                (2 + 14) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:======================================>                 (11 + 5) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample duplicates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 106:>              (0 + 16) / 16][Stage 107:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 106:>              (1 + 15) / 16][Stage 107:>                (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 106:===>                                                   (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 106:=================>                                     (5 + 11) / 16]\r",
      "\r",
      "[Stage 106:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|merchant_id|count|\n",
      "+-----------+-----+\n",
      "|      99621|    4|\n",
      "|      38311|   15|\n",
      "|      68579|    2|\n",
      "|      82529|   17|\n",
      "|      81900|    9|\n",
      "|      66010|  161|\n",
      "|       7833|    3|\n",
      "|      90461|   29|\n",
      "|      25591|    3|\n",
      "|      73470|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:>              (0 + 16) / 16][Stage 118:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:======>                                                (2 + 14) / 16]\r",
      "\r",
      "[Stage 117:=================>                                     (5 + 11) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 120:>                                                      (0 + 16) / 17]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 120:======>                                                (2 + 15) / 17]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. POPULATE dim_merchant\n",
    "\n",
    "# Check for duplicates in the source data\n",
    "duplicate_check = transactions_df.groupBy(\"merchant_id\").count().filter(col(\"count\") > 1)\n",
    "print(f\"Duplicate merchant_ids in transactions: {duplicate_check.count()}\")\n",
    "\n",
    "# Extract unique merchants from transactions\n",
    "dim_merchant_df = transactions_df.select(\n",
    "    col(\"merchant_id\"),\n",
    "    col(\"merchant_id\").alias(\"merchant_sk\"),\n",
    "    col(\"mcc\"),\n",
    "    col(\"merchant_city\"),\n",
    "    col(\"merchant_state\"), \n",
    "    col(\"zip\").alias(\"merchant_zip\")\n",
    ").distinct()  \n",
    "\n",
    "print(f\"Unique merchants found: {dim_merchant_df.count()}\")\n",
    "\n",
    "# Add merchant_type by joining with mcc_codes\n",
    "dim_merchant_df = dim_merchant_df.join(\n",
    "    mcc_codes_df.select(col(\"mcc\"), col(\"merchant_type\")), \n",
    "    on=\"mcc\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check for duplicates after join\n",
    "duplicate_after_join = dim_merchant_df.groupBy(\"merchant_id\").count().filter(col(\"count\") > 1)\n",
    "print(f\"Duplicates after join: {duplicate_after_join.count()}\")\n",
    "\n",
    "# If duplicates exist, show them\n",
    "if duplicate_after_join.count() > 0:\n",
    "    print(\"Sample duplicates:\")\n",
    "    duplicate_after_join.show(10)\n",
    "\n",
    "# Force single partition and ensure no duplicates\n",
    "dim_merchant_df = dim_merchant_df.dropDuplicates([\"merchant_id\"]).coalesce(1)\n",
    "\n",
    "# Write to MySQL\n",
    "# dim_merchant_df.write.jdbc(\n",
    "#     dw_mysql_url, \n",
    "#     \"dim_merchant\", \n",
    "#     mode=\"append\", \n",
    "#     properties={\n",
    "#         **dw_mysql_properties,\n",
    "#         \"batchsize\": \"1000\",\n",
    "#         \"isolationLevel\": \"READ_UNCOMMITTED\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "dim_merchant_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://lakehouse/dim_merchant/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769cc8cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:45:24.975452Z",
     "iopub.status.busy": "2025-10-10T04:45:24.975244Z",
     "iopub.status.idle": "2025-10-10T04:45:25.988729Z",
     "shell.execute_reply": "2025-10-10T04:45:25.987899Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. POPULATE dim_card\n",
    "\n",
    "dim_card_df = cards_df.select(\n",
    "    col(\"card_id\"),\n",
    "    col(\"card_id\").alias(\"card_sk\"),  # Using natural key as surrogate for now\n",
    "    col(\"client_id\"),\n",
    "    col(\"card_brand\"),\n",
    "    col(\"card_type\"),\n",
    "    col(\"credit_limit\"),\n",
    "    col(\"acct_open_date\"),\n",
    "    col(\"has_chip\"),\n",
    "    col(\"card_on_dark_web\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "# dim_card_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_card\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_card_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://lakehouse/dim_card/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a31d02a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:45:25.991595Z",
     "iopub.status.busy": "2025-10-10T04:45:25.991298Z",
     "iopub.status.idle": "2025-10-10T04:45:35.150127Z",
     "shell.execute_reply": "2025-10-10T04:45:35.149375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact table prepared: 13305915 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:>                                                      (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:===>                                                   (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:======>                                                (2 + 14) / 16]\r",
      "\r",
      "[Stage 139:==========>                                            (3 + 13) / 16]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:===============================>                        (9 + 7) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. POPULATE fact_transactions\n",
    "\n",
    "fact_df = transactions_df.select(\n",
    "    col(\"transaction_id\"),\n",
    "    # Create date surrogate key (YYYYMMDD format)\n",
    "    date_format(col(\"trans_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_sk\"),\n",
    "    col(\"client_id\").alias(\"user_sk\"),\n",
    "    col(\"card_id\").alias(\"card_sk\"), \n",
    "    col(\"merchant_id\").alias(\"merchant_sk\"),\n",
    "    col(\"mcc\").alias(\"mcc_sk\"),\n",
    "    col(\"amount\"),\n",
    "    col(\"use_chip\"),\n",
    "    col(\"errors\"),\n",
    "    # No fraud labels - set defaults\n",
    "    lit(0).alias(\"is_fraud\"),\n",
    "    lit(None).cast(\"string\").alias(\"fraud_label_source\"),\n",
    "    lit(1).alias(\"transaction_count\")\n",
    ")\n",
    "\n",
    "print(f\"Fact table prepared: {fact_df.count()} rows\")\n",
    "\n",
    "# Write in batches to avoid memory issues and deadlocks\n",
    "# fact_df.coalesce(5).write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"fact_transactions\") \\\n",
    "#     .option(\"batchsize\", \"50000\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "fact_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://lakehouse/fact_transactions/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
